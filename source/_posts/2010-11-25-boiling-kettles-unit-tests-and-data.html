---
layout: post
title: "Boiling kettles, unit tests and data"
date: 2010-11-25
comments: false
categories:
 - launchpad subunit data
---

<div class='post'>
The Launchpad test suite takes way too long to run: somewhere between three and four hours. &nbsp;Incidentally, <a href="http://rbtcollins.wordpress.com/">Rob</a> has just started <a href="https://dev.launchpad.net/LEP/PersistenceLayer">some work</a> that ought to make it run a lot faster, for which I am truly thankful. &nbsp;Anyway, over the last three (almost four!) years, I've watched the test suite run many, many times, waiting for it to finish or at least reveal a failure.<br /><br />It has been easy to see that not all tests are created equal. Some are very slow, others are very fast. <a href="http://code.mumak.net/2009/09/layers-are-terrible.html">Zope's terrible layer mechanism</a> means that these are often grouped together: slow with slow, fast with fast. Watching the test suite churn away for the umpteenth time made me wonder, exactly how are the tests distributed over time.<br /><br />Luckily, thanks to <a href="https://launchpad.net/subunit">subunit</a>, a <a href="http://paste.ubuntu.com/536368/">bit of Python glue</a>, and OpenOffice it's really easy to get a picture:<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://people.canonical.com/~jml/Tests-finished-each-minute.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="427" src="http://people.canonical.com/~jml/Tests-finished-each-minute.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Graph of how many Launchpad tests are run in each minute of the test run</td></tr></tbody></table>That's a graph of the number of tests that complete within each minute of the Launchpad test suite run. &nbsp;What it means is that you are more likely to see an actual test result if you watch at a random time during the first half of the test run. It also means that we have a relatively small number of very slow tests.<br /><br />Sometimes these graphs are more useful at lower granularity, so I made this one:<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://people.canonical.com/~jml/Tests-finished-each-ten-minutes.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="374" src="http://people.canonical.com/~jml/Tests-finished-each-ten-minutes.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Graph of how many Launchpad tests are run in each ten minute block of the test run</td></tr></tbody></table>And this one:<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://people.canonical.com/~jml/Tests-finished-each-hour.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="390" src="http://people.canonical.com/~jml/Tests-finished-each-hour.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Graph of how many Launchpad tests are run in each hour block of the test run</td></tr></tbody></table><br />The <a href="http://people.canonical.com/~jml/testtools-experiment-r11753.subunit.gz">raw data</a> is taken from a successful run of one of my recent branches. If you download it and open it up, you'll see that the subunit output includes a large number of "time:" statements. Each of these indicates that all subunit statements following it should be considered as taking place at the time given in the original statement.<br /><br />I used a <a href="http://paste.ubuntu.com/536368/">small Python script </a>to generate the CSVs, running it like this:<br /><pre>gzip -dc testtools-experiment-r11753.subunit.gz | python test-distribution.py 60 &gt; /home/jml/Desktop/tests-each-minute.csv</pre><br />Where "60" is the number of seconds of granularity.<br /><br />It was about twenty minutes work all up, and subunit made much of it dead simple. subunit has its flaws, but it's a really good idea.<br /><br />That said, I can't help but feel that it should have been less work.&nbsp;Partly, subunit should have a way to convert data to a format more amenable to analysis by third party tools. Mostly though, those third party tools ought to exist and be known to me.<br /><br />I can easily imagine a tool where I wouldn't have to run the script each time to get a different level of granularity, but rather I could use a slider in a UI and watch the graph update itself in real time. That would be cool, and relatively easy. How come nobody has done that yet?<br /><br />Also, although OpenOffice's graphing thing is fairly nice, why isn't there a better tool? One that makes it easier to share graphs as images on the web without having to take crummy screenshots.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>David Pitkin<span class='comment-date'> on 2012-02-01 18:21</span></div>
<div class='content'>
Closets I know of off the top of my head is the Google Chart API for interactive, animated charts.<br /><br />http://code.google.com/apis/chart/interactive/docs/gallery/controls.html</div>
</div>
</div>
