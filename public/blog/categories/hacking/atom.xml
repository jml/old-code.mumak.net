<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hacking | Mere Code]]></title>
  <link href="http://code.mumak.net/blog/categories/hacking/atom.xml" rel="self"/>
  <link href="http://code.mumak.net/"/>
  <updated>2013-09-13T23:05:41+01:00</updated>
  <id>http://code.mumak.net/</id>
  <author>
    <name><![CDATA[Jonathan Lange]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[User Experience — When Reality Attacks]]></title>
    <link href="http://code.mumak.net/2008/03/user-experience-when-reality-attacks_12.html"/>
    <updated>2008-03-12T00:00:00+00:00</updated>
    <id>http://code.mumak.net/2008/03/user-experience-when-reality-attacks_12</id>
    <content type="html"><![CDATA[<div class='post'>
I've just got back from a hectic week in London, where members of the Bazaar community got together and thrashed out a bunch of important topics.<br /><br />We talked about "user experience" and how we all want Bazaar to be a joy to use. More than one person said that we have been focusing too much on features and performance instead of user experience. The term was never really pinned down, but it's fair to say that there are things other than convenience and speed that affect how users feel while using Bazaar and that we need to work on those things, once we figure out what they are.<br /><br />I think I might know the name of one of them: <em>errors</em>. Next post: "Notes on error".</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Within You Without You]]></title>
    <link href="http://code.mumak.net/2007/08/within-you-without-you.html"/>
    <updated>2007-08-11T00:00:00+01:00</updated>
    <id>http://code.mumak.net/2007/08/within-you-without-you</id>
    <content type="html"><![CDATA[<div class='post'>
Testing is hard, writing testing frameworks is easy. In an effort to make testing easier, big projects like Twisted, Bazaar and Zope write their own testing frameworks. That way they control both the test runner and the tests that are run. It's actually quite convenient.<br /><br />However, it's led to a significant problem:<br /><blockquote>There are many similar implementations of xUnit in Python, each with subtle incompatibilities.</blockquote><br />Running Twisted tests in the Zope test runner? Watch out for the threads that the reactor maintains between tests. Running Bazaar tests with Trial? On my machine, I get told that elementtree doesn't have an 'ElementTree' attribute. Hmm.<br /><br />When talking about this problem, I often refer loosely to "PyUnit compatibilty". The idea is that:<br /><ol><br /> <li>Every Python test runner should support running vanilla Python standard library <code>unittest.TestCase</code> tests.</li><br />    <li>Every Python unit test should be able to be run using the mechanisms in <code>unittest.py</code> in the Python standard library.</li><br /></ol><br />In other words, this code should Just Work:<br /><pre lang="python" line="1"><br />import unittest<br />from yourframework import testing<br /><br />class PythonTestCase(unittest.TestCase):<br />    def test_something(self):<br />        pass<br /><br />class FrameworkTestCase(testing.TestCase):<br />    def test_something(self):<br />        pass<br /><br />if __name__ == '__main__':<br />    python_test_result = unittest.TestResult()<br />    framework_test_result = testing.TestResult()<br />    FrameworkTestCase('test_something').run(python_test_result)<br />    FrameworkTestCase('test_something').run(framework_test_result)<br />    PythonTestCase('test_something').run(python_test_result)<br />    PythonTestCase('test_something').run(framework_test_result)<br />    # At this point, python_test_result and framework_test_result<br />    # should hold equivalent data.</pre><br />If your framework is PyUnit compatible then the above fragment should give the same results if run directly or if run in your runner. Things get a little bit hazier when it comes to test discovery.<br /><br />So, if your unit test requires that it be run inside a special suite (e.g. <code>TrialSuite</code>) in order to work correctly, it is not PyUnit compatible. If your test runner does some critical set up that enables features that your tests need, then it is not PyUnit compatible.<br /><br />This leads to a kind of thinking where certain features belong on the base test case and others belong in the test runner. Putting features in the wrong place might not lead to a strict incompatibility, but it can lead to significant inconvenience. (And what are automated tests if not a convenience?).<br /><br />Two examples from Twisted:<br /><br /><strong>Temporary Working Directory</strong><br /><br />Trial creates a <code>_trial_temp</code> working directory and changes into that directory to run tests. In Trial, this feature is provided by the test runner. It should be provided by the base <code>TestCase</code> class.<br /><ul><br />   <li>It's not clear that every test needs this feature.</li><br />   <li>Twisted tests now assume that they can create files with impunity. When Twisted tests are run in a different test runner, they leave garbage files everywhere.</li><br /></ul><br /><strong>Timeouts</strong><br /><br />By default, any Trial test that runs for more than two minutes will fail with a timeout error. The timeout period can be configured on a per-test, per-test-class, per-module or per-package basis. Trial implements this feature on <code>TestCase</code>, it should be implemented on the runner.<br /><ul><br />    <li>Even in the Twisted test runner, this makes debugging more painful. You must do all of your debugging in under two minutes.</li><br />  <li>Intuitively, you might think that the <em>runner</em> should control how tests are <em>run</em>.</li><br /> <li>Tests that don't descend from Trial's <code>TestCase</code> can still hang.</li><br />  <li>Two minutes might be good enough for me on a Monday, but I might be busier on Friday. I should be able to change the timeout without changing code.</li><br /></ul></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Heartbeats and Sails]]></title>
    <link href="http://code.mumak.net/2007/08/heartbeats-and-sails.html"/>
    <updated>2007-08-04T00:00:00+01:00</updated>
    <id>http://code.mumak.net/2007/08/heartbeats-and-sails</id>
    <content type="html"><![CDATA[<div class='post'>
<a href="http://www.markshuttleworth.com/archives/125">Mark Shuttleworth:</a><br /><blockquote>What’s good enough performance? Well, I like to think in terms of “heartbeat time”. If the major operations which I have to do regularly (several times in an hour) take less than a heartbeat, then I don’t ever feel like I’m waiting. Things which happen 3-5 times in a day can take a bit longer, up to a minute, and those fit with regular workbreaks that I would take anyhow to clear my head for the next phase of work, or rest my aching fingers.</blockquote><br />Take this rule of thumb and apply it to unit tests:<br /><ul><br /> <li>Tests for whatever chunk of code you are working on should take "less than a heartbeat".</li><br /> <li>Your <em>entire</em> testing suite (that you run 3-5 times in a day, right?) can take longer to run, up to a minute.</li><br /></ul><br />Authors of tests and testing frameworks, there's your challenge.<br /><br />Tests that take too long to run just won't get run. Programmers will postpone running the suite until the last possible moment. When using something like <a href="https://launchpad.net/pqm">PQM</a> or <a href="http://buildbot.net">Buildbot</a>, this can be disastrous. Other developers might have to wait hours for their code to land on trunk.<br /><br />Gerard Mezsaros's new book, <a href="http://xunitpatterns.com/"><em>xUnit Test Patterns</em></a> has some good ideas about what to do and what <em>not</em> to do to make your tests run in a couple of heartbeats.</div>

]]></content>
  </entry>
  
</feed>
